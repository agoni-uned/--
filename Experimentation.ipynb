{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import plotly.io as pio\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../DMR3/'   # Set path to the directory contaning the data\n",
    "# Data should have been already cleaned. Data should be organized as follows:\n",
    "    # Images should be stored sorted by diagnosis and view. For instance, the directory images/healthy/front/ contains frontal images of healthy subjects\n",
    "    # Clinical data should should be in a csv file, where each, with columns being the different features and rows the different subjects. \n",
    "    #     There should be separate files for the healthy and sick groups, and they should be places within a directory called clinical_data/: \n",
    "    #     clinical_data/clinical_data_h.csv for healthy patients and clinical_data/clinical_data_s.csv for sick patients\n",
    "\n",
    "\n",
    "os.mkdir(path + 'Models')\n",
    "os.mkdir(path + 'Learning_curves')\n",
    "os.mkdir(path + 'Predictions')\n",
    "os.mkdir(path + 'Param_tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-celtic",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-kinase",
   "metadata": {},
   "source": [
    "## Thermal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frontal images\n",
    "front_images_h = np.asarray([cv2.imread(img,0) for img in os.listdir(path+'images/healthy/front')])\n",
    "front_images_s = np.asarray([cv2.imread(img,0) for img in os.listdir(path+'images/sick/front')])\n",
    "front_labels = [0]*len(front_images_h) + [1]*len(front_images_s)\n",
    "\n",
    "## Left lateral (L90) images\n",
    "L90_images_h = np.asarray([cv2.imread(img,0) for img in os.listdir(path+'images/healthy/L90')])\n",
    "L90_images_s = np.asarray([cv2.imread(img,0) for img in os.listdir(path+'images/sick/L90')])\n",
    "L90_labels = [0]*len(L90_images_h) + [1]*len(L90_images_s)\n",
    "\n",
    "## Right lateral (R90) images\n",
    "R90_images_h = np.asarray([cv2.imread(img,0) for img in os.listdir(path+'images/healthy/R90')])\n",
    "R90_images_s = np.asarray([cv2.imread(img,0) for img in os.listdir(path+'images/sick/R90')])\n",
    "R90_labels = [0]*len(R90_images_h) + [1]*len(R90_images_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of thermograms\n",
    "_,w,h = front_images_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-screen",
   "metadata": {},
   "source": [
    "## Labels (diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_h = [0]*len(front_images_h)\n",
    "labels_s = [1]*len(front_images_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-pathology",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data_h = pd.read_csv(path+'clinical_data/clinical_data_h.csv')\n",
    "clinical_data_s = pd.read_csv(path+'clinical_data/clinical_data_s.csv')\n",
    "cd_colnames = clinical_data_h.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-camel",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-cheat",
   "metadata": {},
   "source": [
    "## Thermal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min-max normalization\n",
    "M = np.concatenate((front_images_h,front_images_s,L90_images_h,L90_images_s,R90_images_h,R90_images_s)).max()\n",
    "m = np.concatenate((front_images_h,front_images_s,L90_images_h,L90_images_s,R90_images_h,R90_images_s)).min()\n",
    "\n",
    "front_images_h = ((front_images_h - m) / (M - m)).astype('float32')\n",
    "front_images_s = ((front_images_s - m) / (M - m)).astype('float32')\n",
    "\n",
    "L90_images_h = ((L90_images_h - m) / (M - m)).astype('float32')\n",
    "L90_images_s = ((L90_images_s - m) / (M - m)).astype('float32')\n",
    "\n",
    "R90_images_h = ((R90_images_h - m) / (M - m)).astype('float32')\n",
    "R90_images_s = ((R90_images_s - m) / (M - m)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-count",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize\n",
    "M = pd.concat([clinical_data_h,clinical_data_s]).max().values\n",
    "M[M<1] = 1\n",
    "m = pd.concat([clinical_data_h,clinical_data_s]).min().values\n",
    "\n",
    "clinical_data_h = (clinical_data_h-m)/(M-m)\n",
    "clinical_data_s = (clinical_data_s-m)/(M-m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select columns\n",
    "id_columns_to_delete = [1, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, 18, 22, 23, 25]\n",
    "columns_to_delete = cd_colnames[id_columns_to_delete]\n",
    "clinical_data_h.drop(columns_to_delete,axis=1,inplace=True)\n",
    "clinical_data_s.drop(columns_to_delete,axis=1,inplace=True)\n",
    "cd_colnames = list(cd_colnames)\n",
    "for f in columns_to_delete:\n",
    "    cd_colnames.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to numpy array\n",
    "clinical_data_h = np.asarray(clinical_data_h, dtype=np.float32)\n",
    "clinical_data_s = np.asarray(clinical_data_s, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-liabilities",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(front_images_h,front_images_s,L90_images_h,L90_images_s,R90_images_h,R90_images_s,clinical_data_h,clinical_data_s,split_rate=0.15):   \n",
    "    \n",
    "    ## Randomly shuffle the data\n",
    "    N_h = len(front_images_h)\n",
    "    indices_h = np.random.permutation(N_h)\n",
    "    front_images_h_shuffled = front_images_h[indices_h]\n",
    "    L90_images_h_shuffled = L90_images_h[indices_h]\n",
    "    R90_images_h_shuffled = R90_images_h[indices_h]\n",
    "    clinical_data_h_shuffled = clinical_data_h[indices_h]\n",
    "\n",
    "    N_s = len(front_images_s)\n",
    "    indices_s = np.random.permutation(N_s)\n",
    "    front_images_s_shuffled = front_images_s[indices_s]\n",
    "    L90_images_s_shuffled = L90_images_s[indices_s]\n",
    "    R90_images_s_shuffled = R90_images_s[indices_s]\n",
    "    clinical_data_s_shuffled = clinical_data_s[indices_s]\n",
    "\n",
    "    ## split healthy and sick sets in train and test\n",
    "    split_index_h = np.round(N_h * split_rate).astype(int)\n",
    "    split_index_s = np.round(N_s * split_rate).astype(int)\n",
    "\n",
    "    train_front_h = front_images_h_shuffled[split_index_h : ]\n",
    "    test_front_h = front_images_h_shuffled[ : split_index_h]\n",
    "    train_L90_h = L90_images_h_shuffled[split_index_h : ]\n",
    "    test_L90_h = L90_images_h_shuffled[ : split_index_h]\n",
    "    train_R90_h = R90_images_h_shuffled[split_index_h : ]\n",
    "    test_R90_h = R90_images_h_shuffled[ : split_index_h]\n",
    "    train_cd_h = clinical_data_h_shuffled[split_index_h : ]\n",
    "    test_cd_h = clinical_data_h_shuffled[ : split_index_h]\n",
    "\n",
    "    train_front_s = front_images_s_shuffled[split_index_s : ]\n",
    "    test_front_s = front_images_s_shuffled[ : split_index_s]\n",
    "    train_L90_s = L90_images_s_shuffled[split_index_s : ]\n",
    "    test_L90_s = L90_images_s_shuffled[ : split_index_s]\n",
    "    train_R90_s = R90_images_s_shuffled[split_index_s : ]\n",
    "    test_R90_s = R90_images_s_shuffled[ : split_index_s]\n",
    "    train_cd_s = clinical_data_s_shuffled[split_index_s : ]\n",
    "    test_cd_s = clinical_data_s_shuffled[ : split_index_s]\n",
    "    \n",
    "    ## create labels\n",
    "    train_labels_h = [0]*len(train_front_h)\n",
    "    test_labels_h = [0]*len(test_front_h)\n",
    "\n",
    "    train_labels_s = [1]*len(train_front_s)\n",
    "    test_labels_s = [1]*len(test_front_s)\n",
    "    \n",
    "    ## create train and test sets\n",
    "    train_front = np.concatenate((train_front_h, train_front_s))\n",
    "    train_L90 = np.concatenate((train_L90_h, train_L90_s))\n",
    "    train_R90 = np.concatenate((train_R90_h, train_R90_s))\n",
    "    train_cd = np.concatenate((train_cd_h, train_cd_s))\n",
    "    train_labels = np.concatenate((train_labels_h, train_labels_s))\n",
    "    \n",
    "    test_front = np.concatenate((test_front_h, test_front_s))\n",
    "    test_L90 = np.concatenate((test_L90_h, test_L90_s))\n",
    "    test_R90 = np.concatenate((test_R90_h, test_R90_s))\n",
    "    test_cd = np.concatenate((test_cd_h, test_cd_s))\n",
    "    test_labels = np.concatenate((test_labels_h, test_labels_s))\n",
    "    \n",
    "    ## Randomly shuffle data\n",
    "    indices_train = np.random.permutation(len(train_labels))\n",
    "    train_front = train_front[indices_train]\n",
    "    train_L90 = train_L90[indices_train]\n",
    "    train_R90 = train_R90[indices_train]\n",
    "    train_cd = train_cd[indices_train]\n",
    "    train_labels = train_labels[indices_train]\n",
    "    \n",
    "    indices_test = np.random.permutation(len(test_labels))\n",
    "    test_front = test_front[indices_test]\n",
    "    test_L90 = test_L90[indices_test]\n",
    "    test_R90 = test_R90[indices_test]\n",
    "    test_cd = test_cd[indices_test]\n",
    "    test_labels = test_labels[indices_test]\n",
    "    \n",
    "    train_data = [train_front,train_L90,train_R90,train_cd,train_labels]\n",
    "    test_data = [test_front,test_L90,test_R90,test_cd,test_labels]\n",
    "    \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = split_data(front_images_h,front_images_s,L90_images_h,L90_images_s,R90_images_h,R90_images_s,clinical_data_h,clinical_data_s,split_rate=0.15)\n",
    "train_front,train_L90,train_R90,train_cd,train_labels = train_data\n",
    "test_front,test_L90,test_R90,test_cd,test_labels = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sick = train_labels.sum()\n",
    "n_healthy = len(train_labels) - n_sick\n",
    "rate_train = n_healthy / n_sick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-university",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "EPOCHS = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "def weighted_error(y_test, y_test_pred, rate=20):\n",
    "            \n",
    "    # False negatives (y = 1, y_pred = 0)\n",
    "    fn = np.sum(np.greater(y_test, y_test_pred))\n",
    "\n",
    "    # False positives (y = 0, y_pred = 1)\n",
    "    fp = np.sum(np.less(y_test, y_test_pred))\n",
    "    \n",
    "    return fn*rate + fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_history,val_history,num_epochs):\n",
    "    font_title = 12\n",
    "    font_legend = 10\n",
    "    \n",
    "    # Visualize the training results\n",
    "    epochs_range = range(num_epochs)\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    \n",
    "    train_loss,train_acc,train_AUC = train_history\n",
    "    val_loss,val_acc,val_AUC = val_history\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Test Loss')\n",
    "    plt.legend(loc='upper right',fontsize=font_legend)\n",
    "    plt.title('Train and Test Loss',fontsize=font_title)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range, train_acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right',fontsize=font_legend)\n",
    "    plt.title('Train and Test Accuracy',fontsize=font_title)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs_range, train_AUC, label='Train AUC')\n",
    "    plt.plot(epochs_range, val_AUC, label='Test AUC')\n",
    "    plt.legend(loc='lower right',fontsize=font_legend)\n",
    "    plt.title('Train and Test ROC AUC',fontsize=font_title)\n",
    "\n",
    "    #plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def numeric_results(ground_truth,preds_raw):\n",
    "    \n",
    "    # Binary Cross Entropy\n",
    "    bce = tf.keras.metrics.BinaryCrossentropy  ()\n",
    "    bce.update_state(ground_truth, preds_raw)\n",
    "    \n",
    "    # ROC AUC\n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    auc.update_state(ground_truth, preds_raw)\n",
    "    \n",
    "    # Precision\n",
    "    precision = tf.keras.metrics.Precision()\n",
    "    precision.update_state(ground_truth, preds_raw)\n",
    "    \n",
    "    # Recall\n",
    "    recall = tf.keras.metrics.Recall()\n",
    "    recall.update_state(ground_truth, preds_raw)\n",
    "    \n",
    "    TN, FP, FN, TP = confusion_matrix(ground_truth, np.round(preds_raw)).ravel()\n",
    "    results = {\n",
    "        'BCELoss':bce.result().numpy(),\n",
    "        'Accuracy':accuracy_score(ground_truth, np.round(preds_raw)),\n",
    "        'TP':TP,\n",
    "        'FP':FP,\n",
    "        'TN':TN,\n",
    "        'FN':FN,\n",
    "        'sensitivity':TP/(TP+FN),\n",
    "        'specificity':TN/(TN+FP),\n",
    "        'G-mean':np.sqrt((TP/(TP+FN))*(TN/(TN+FP))),\n",
    "        'precision':precision.result().numpy(),\n",
    "        'recall':recall.result().numpy(),\n",
    "        'F1':2*precision.result().numpy()*recall.result().numpy()/(precision.result().numpy()+recall.result().numpy()),\n",
    "        'ROC AUC':auc.result().numpy(),\n",
    "        'WE':weighted_error(ground_truth, np.round(preds_raw))\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstraping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def bootstrapping(y_true,y_pred):\n",
    "    # Bootstrap resampling\n",
    "    n_bootstraps = 1000  # Number of bootstrap samples\n",
    "    auc_scores = []      # List to store AUC scores from each bootstrap sample\n",
    "    rng = np.random.RandomState(42)  # Random state for reproducibility\n",
    "    \n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    \n",
    "    # Compute the AUC on the original data\n",
    "    auc.update_state(y_true, y_pred)\n",
    "    observed_auc = auc.result().numpy()\n",
    "    \n",
    "    count = 0\n",
    "    while count < n_bootstraps:\n",
    "    #for _ in range(n_bootstraps):\n",
    "        # Generate bootstrap sample indices\n",
    "        indices = rng.choice(len(y_true), size=len(y_true), replace=True)\n",
    "\n",
    "        #if len(np.unique(y_true[indices])==1): print('A')\n",
    "        if len(np.unique(y_true[indices]))> 1:\n",
    "        \n",
    "            # Compute AUC on bootstrap sample\n",
    "            auc.update_state(y_true[indices], y_pred[indices])\n",
    "            auc_bootstrap = auc.result().numpy()\n",
    "            auc_scores.append(auc_bootstrap)\n",
    "            \n",
    "            count = count+1\n",
    "            \n",
    "    # Calculate statistics from bootstrap samples\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    ci_lower, ci_upper = np.percentile(auc_scores, [2.5, 97.5])  # 95% confidence interval\n",
    "    \n",
    "    # Calculate p-value (two-tailed)\n",
    "    p_value = np.mean(np.abs(auc_scores - observed_auc) >= np.abs(observed_auc))\n",
    "\n",
    "    print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "    print(f\"Standard Deviation of AUC: {std_auc:.4f}\")\n",
    "    print(f\"95% Confidence Interval of AUC: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "    print(f\"p-value of AUC: {p_value:.4f}\")\n",
    "    \n",
    "    return mean_auc,std_auc,ci_lower,ci_upper,p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the number of parameters of each model\n",
    "def num_parameters_classifiers(classifier_type,cd_classifier):\n",
    "        \n",
    "    if classifier_type == 'WeightedVoting':\n",
    "        n_parameters = 1\n",
    "        \n",
    "    if classifier_type == 'SVC':\n",
    "        n_support_vectors = len(cd_classifier.support_vectors_)\n",
    "        n_coefficients = len(cd_classifier.dual_coef_[0])\n",
    "        n_parameters = n_support_vectors + n_coefficients\n",
    "        \n",
    "    elif classifier_type == 'DT':\n",
    "        n_parameters = cd_classifier.tree_.node_count\n",
    "        \n",
    "    elif classifier_type == 'RF':\n",
    "        n_trees = len(cd_classifier.estimators_)\n",
    "        n_parameters = sum(tree.tree_.node_count for tree in cd_classifier.estimators_)\n",
    "        \n",
    "    elif classifier_type == 'DT_AdaBoost':\n",
    "        n_estimators = len(cd_classifier.estimators_)\n",
    "        n_parameters = sum(estimator.tree_.node_count for estimator in cd_classifier.estimators_)\n",
    "        \n",
    "    elif classifier_type == 'RF_AdaBoost':\n",
    "        n_estimators = len(cd_classifier.estimators_)\n",
    "        n_parameters = sum(\n",
    "            sum(tree.tree_.node_count for tree in estimator.estimators_)\n",
    "            for estimator in cd_classifier.estimators_\n",
    "        )\n",
    "        \n",
    "    elif classifier_type == 'SGD':\n",
    "            n_parameters = classifier.coef_.size + classifier.intercept_.size\n",
    "            \n",
    "    elif classifier_type == 'NN':\n",
    "        n_parameters = classifier.coef_.size + classifier.intercept_.size\n",
    "\n",
    "    else:\n",
    "        print('Error')\n",
    "        n_parameters = None\n",
    "        \n",
    "    return n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-mambo",
   "metadata": {},
   "source": [
    "## Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4  # k-fold cross-validation\n",
    "n_trials = 30  # Trials for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colnames(param_names):\n",
    "    colnames = ['Model','k','target'] + param_names + ['tuningTime','bestValue']\n",
    "    return colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crossval_subsets(front,L90,R90,cd,labels,k):\n",
    "    # Create train+val subsets for k-fold cross-validation\n",
    "    fold_size = len(labels) // k\n",
    "    \n",
    "    train_val_subsets = []\n",
    "    for i in range(k):\n",
    "        val_front = front[i*fold_size:(i+1)*fold_size]\n",
    "        val_L90 = L90[i*fold_size:(i+1)*fold_size]\n",
    "        val_R90 = R90[i*fold_size:(i+1)*fold_size]\n",
    "        val_cd = cd[i*fold_size:(i+1)*fold_size]\n",
    "        val_labels = labels[i*fold_size:(i+1)*fold_size]\n",
    "        \n",
    "        train_front = np.concatenate((front[0:i*fold_size],front[(i+1)*fold_size::]))\n",
    "        train_L90 = np.concatenate((L90[0:i*fold_size],L90[(i+1)*fold_size::]))\n",
    "        train_R90 = np.concatenate((R90[0:i*fold_size],R90[(i+1)*fold_size::]))\n",
    "        train_cd = np.concatenate((cd[0:i*fold_size],cd[(i+1)*fold_size::]))\n",
    "        train_labels = np.concatenate((labels[0:i*fold_size],labels[(i+1)*fold_size::]))\n",
    "        \n",
    "        train_data = [train_front,train_L90,train_R90,train_cd,train_labels]\n",
    "        val_data = [val_front,val_L90,val_R90,val_cd,val_labels]\n",
    "        \n",
    "        train_val_subsets.append([train_data,val_data])\n",
    "    \n",
    "    return train_val_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def classifier_tuneHyperparameters(estimator,param_grid,X_train,y_train):\n",
    "    #cost_scorer = make_scorer(weighted_error, greater_is_better=False)\n",
    "    cost_scorer = 'roc_auc'\n",
    "\n",
    "    # Tune hyperparameters with 4-fold cross-validation on training set\n",
    "    classifier = GridSearchCV(estimator, param_grid, scoring=cost_scorer, cv=4).fit(X_train, y_train)\n",
    "    parameters = classifier.best_params_\n",
    "    results_cv = classifier.cv_results_\n",
    "    \n",
    "    return classifier, parameters, results_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-parker",
   "metadata": {},
   "source": [
    "# Clinical data distribution across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {feat : np.unique(np.concatenate((clinical_data_h,clinical_data_s))[:,i]) for i,feat in enumerate(cd_colnames) if 'age' not in feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_healthy_train = sum(train_labels==0)\n",
    "n_sick_train = sum(train_labels==1)\n",
    "n_healthy_test = sum(test_labels==0)\n",
    "n_sick_test = sum(test_labels==1)\n",
    "    \n",
    "count_train_h,count_train_s,count_test_h,count_test_s = {},{},{},{}\n",
    "for i,feat in enumerate(cd_colnames):\n",
    "    if 'age' not in feat:\n",
    "        # Categorical feature\n",
    "        for v in values[feat]:\n",
    "            count_train_h[feat+': '+str(v)+' (%)'] = sum(train_cd[train_labels==0][:,i] == v) / n_healthy_train\n",
    "            count_test_h[feat+': '+str(v)+' (%)'] = sum(test_cd[test_labels==0][:,i] == v) / n_healthy_test\n",
    "            count_train_s[feat+': '+str(v)+' (%)'] = sum(train_cd[train_labels==1][:,i] == v) / n_sick_train\n",
    "            count_test_s[feat+': '+str(v)+' (%)'] = sum(test_cd[test_labels==1][:,i] == v) / n_sick_test\n",
    "    else:\n",
    "        # Numerical feature\n",
    "        count_train_h[feat+' [mean,std]'] = [train_cd[train_labels==0][:,i].mean(),train_cd[train_labels==0][:,i].std()]\n",
    "        count_test_h[feat+' [mean,std]'] = [test_cd[test_labels==0][:,i].mean(),test_cd[test_labels==0][:,i].std()]\n",
    "        count_train_s[feat+' [mean,std]'] = [train_cd[train_labels==1][:,i].mean(),train_cd[train_labels==1][:,i].std()]\n",
    "        count_test_s[feat+' [mean,std]'] = [test_cd[test_labels==1][:,i].mean(),test_cd[test_labels==1][:,i].std()]\n",
    "            \n",
    "distribution = {'train_healthy' : count_train_h,\n",
    "                'test_healthy' : count_test_h,\n",
    "                'train_sick' : count_train_s,\n",
    "                'test_sick' : count_test_s}\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "print(pd.DataFrame(distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-organ",
   "metadata": {},
   "source": [
    "# Clinical Data (CD) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['SVC','DT','RF','DT_AdaBoost','RF_AdaBoost']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-ethnic",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_params = {}\n",
    "for classifier_type in models:\n",
    "    print(classifier_type)\n",
    "    if classifier_type == 'SVC':\n",
    "        # Support Vector Machine (SVM) classifier\n",
    "        estimator = SVC(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = [{'C': range(1, 1000, 10), 'kernel': ['linear', 'rbf', 'sigmoid'], 'gamma': ['scale', 'auto', 0.001, 0.0001, 0.00001]}]\n",
    "        \n",
    "    elif classifier_type in ['DT','DT_AdaBoost']:\n",
    "        # Decision Tree (DT) classifier\n",
    "        estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = [{'ccp_alpha' : np.arange(0, 0.1, 0.005),\n",
    "                       'criterion': [\"gini\"],\n",
    "                       'max_depth' : [None, 1, 10, 15],\n",
    "                       'max_features': ['auto'],\n",
    "                       'max_leaf_nodes': [None, 3, 6, 9],\n",
    "                       'min_samples_leaf': [2, 3, 4],\n",
    "                       'min_samples_split' : [3, 10, 15],\n",
    "                       'min_weight_fraction_leaf' : np.arange(0.1, 0.5, 0.01),\n",
    "                       'splitter': ['best']}]\n",
    "\n",
    "    else:\n",
    "        # Random Forest (RF) classifier\n",
    "        estimator = RandomForestClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],  # Number of trees in random forest\n",
    "                      'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at every split\n",
    "                      'max_depth': [int(x) for x in np.linspace(10, 110, num=11)]+['None'],  # Maximum number of levels in tree\n",
    "                      'min_samples_split': [2, 3, 5, 10],  # Minimum number of samples required to split a node\n",
    "                      'min_samples_leaf': [1, 2, 3, 4],  # Minimum number of samples required at each leaf node\n",
    "                      'bootstrap': [True, False]}  # Method of selecting samples for training each tree\n",
    "    \n",
    "    classifier, parameters, results_cv = classifier_tuneHyperparameters(estimator, param_grid, train_cd, train_labels)\n",
    "    cd_params[classifier_type] = parameters\n",
    "    \n",
    "    print('Tuned parameters:'), print(parameters), print()\n",
    "    \n",
    "    # Evaluate\n",
    "    print('TRAIN:'), print(numeric_results(train_labels,classifier.predict(train_cd)))\n",
    "    print('TEST:'), print(numeric_results(test_labels,classifier.predict(test_cd)))\n",
    "    \n",
    "    print('#'*150), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-realtor",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cd_classifier(train_cd,train_labels,test_cd,test_labels,classifier_type,parameters):  \n",
    "    \n",
    "    n_sick = train_labels.sum()\n",
    "    n_healthy = len(train_labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "    \n",
    "    ## Initialize the estimator\n",
    "    if classifier_type == 'SVC':\n",
    "        # Support Vector Machine (SVM) classifier\n",
    "        estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "        \n",
    "    elif classifier_type in ['DT','DT_AdaBoost']:\n",
    "        # Decision Tree (DT) classifier\n",
    "        #parameters = {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 3, \n",
    "        #              'min_samples_split': 10, 'min_weight_fraction_leaf': 0.13999999999999999, 'splitter': 'best'}\n",
    "        estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "    else:\n",
    "        # Random Forest (RF) classifier\n",
    "        #parameters = {'n_estimators': 1200, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 110, 'bootstrap': False}\n",
    "        estimator = RandomForestClassifier(class_weight={0: 1, 1: rate_train})\n",
    "            \n",
    "    estimator.set_params(**parameters)  \n",
    "        \n",
    "    ## Train the classifier\n",
    "    tic = time.time()\n",
    "    if 'AdaBoost' in classifier_type:\n",
    "        classifier = AdaBoostClassifier(estimator).fit(train_cd, train_labels)\n",
    "    else:\n",
    "        classifier = estimator.fit(train_cd, train_labels)\n",
    "    train_time = time.time() - tic\n",
    "\n",
    "    ## Predictions\n",
    "    tic = time.time()\n",
    "    predictions_train = classifier.predict_proba(train_cd)[:,1]\n",
    "    inference_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    predictions_test = classifier.predict_proba(test_cd)[:,1]\n",
    "    inference_time = inference_time + (time.time()-tic)\n",
    "\n",
    "    inference_time = inference_time/(len(predictions_train)+len(predictions_test))\n",
    "        \n",
    "    return classifier,train_time,inference_time,predictions_train,predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-cincinnati",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train classifiers\n",
    "for classifier_type in models:\n",
    "    print(classifier_type), print()\n",
    "        \n",
    "    ## Train the classifier\n",
    "    classifier,train_time,inference_time,predictions_train,predictions_test = train_cd_classifier(train_cd,train_labels,\n",
    "                                                                                                  test_cd,test_labels,\n",
    "                                                                                                  classifier_type,{})#cd_params[classifier_type])\n",
    "    \n",
    "    ## Save the model\n",
    "    with open(path+'Models/cd_'+classifier_type+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "        \n",
    "    np.save(path+'Predictions/cd_'+classifier_type+'_train.npy',predictions_train)\n",
    "    np.save(path+'Predictions/cd_'+classifier_type+'_test.npy',predictions_test)\n",
    "    \n",
    "    print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "    print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Bootstrapping')\n",
    "    print(bootstrapping(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Number of parameters: ', num_parameters_classifiers(classifier_type,classifier))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-costs",
   "metadata": {},
   "source": [
    "## Select best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_model = 'DT_AdaBoost'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-services",
   "metadata": {},
   "source": [
    "# Thermal image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.layers import Flatten, concatenate, RepeatVector\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "from tensorflow.keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils import metrics_utils\n",
    "from tensorflow.python.keras.utils.generic_utils import to_list\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "class Specificity(tf.keras.metrics.Metric):\n",
    "    \"\"\"Computes the specificity of the predictions with respect to the labels.\n",
    "\n",
    "    This metric creates two local variables, `true_negatives` and\n",
    "    `false_positives`, that are used to compute the specificity. This value is\n",
    "    ultimately returned as `specificity`, an idempotent operation that simply divides\n",
    "    `true_negatives` by the sum of `true_negatives` and `false_positives`.\n",
    "\n",
    "    If `sample_weight` is `None`, weights default to 1.\n",
    "    Use `sample_weight` of 0 to mask values.\n",
    "\n",
    "    If `top_k` is set, recall will be computed as how often on average a class\n",
    "    among the labels of a batch entry is in the top-k predictions.\n",
    "\n",
    "    If `class_id` is specified, we calculate specificity by considering only the\n",
    "    entries in the batch for which `class_id` is in the label, and computing the\n",
    "    fraction of them for which `class_id` is above the threshold and/or in the\n",
    "    top-k predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               thresholds=None,\n",
    "               top_k=None,\n",
    "               class_id=None,\n",
    "               name='specificity',\n",
    "               dtype=None):\n",
    "        \"\"\"Creates a `specificity` instance.\n",
    "\n",
    "        Args:\n",
    "          thresholds: (Optional) A float value or a python list/tuple of float\n",
    "            threshold values in [0, 1]. A threshold is compared with prediction\n",
    "            values to determine the truth value of predictions (i.e., above the\n",
    "            threshold is `true`, below is `false`). One metric value is generated\n",
    "            for each threshold value. If neither thresholds nor top_k are set, the\n",
    "            default is to calculate recall with `thresholds=0.5`.\n",
    "          top_k: (Optional) Unset by default. An int value specifying the top-k\n",
    "            predictions to consider when calculating recall.\n",
    "          class_id: (Optional) Integer class ID for which we want binary metrics.\n",
    "            This must be in the half-open interval `[0, num_classes)`, where\n",
    "            `num_classes` is the last dimension of predictions.\n",
    "          name: (Optional) string name of the metric instance.\n",
    "          dtype: (Optional) data type of the metric result.\n",
    "        \"\"\"\n",
    "        super(Specificity, self).__init__(name=name, dtype=dtype)\n",
    "        self.init_thresholds = thresholds\n",
    "        self.top_k = top_k\n",
    "        self.class_id = class_id\n",
    "\n",
    "        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
    "        self.thresholds = metrics_utils.parse_init_thresholds(\n",
    "            thresholds, default_threshold=default_threshold)\n",
    "        self.true_negatives = self.add_weight(\n",
    "            'true_negatives',\n",
    "            shape=(len(self.thresholds),),\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        self.false_positives = self.add_weight(\n",
    "            'false_positives',\n",
    "            shape=(len(self.thresholds),),\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Accumulates true negative and false positive statistics.\n",
    "\n",
    "        Args:\n",
    "          y_true: The ground truth values, with the same dimensions as `y_pred`.\n",
    "            Will be cast to `bool`.\n",
    "          y_pred: The predicted values. Each element must be in the range `[0, 1]`.\n",
    "          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
    "            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
    "            be broadcastable to `y_true`.\n",
    "\n",
    "        Returns:\n",
    "          Update op.\n",
    "        \"\"\"\n",
    "        return metrics_utils.update_confusion_matrix_variables(\n",
    "            {\n",
    "                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,\n",
    "                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives\n",
    "            },\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            thresholds=self.thresholds,\n",
    "            top_k=self.top_k,\n",
    "            class_id=self.class_id,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        result = math_ops.div_no_nan(self.true_negatives,\n",
    "                                     self.true_negatives + self.false_positives)\n",
    "        return result[0] if len(self.thresholds) == 1 else result\n",
    "\n",
    "    def reset_states(self):\n",
    "        num_thresholds = len(to_list(self.thresholds))\n",
    "        K.batch_set_value(\n",
    "            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'thresholds': self.init_thresholds,\n",
    "            'top_k': self.top_k,\n",
    "            'class_id': self.class_id\n",
    "        }\n",
    "        base_config = super(Specificity, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class WeightedError(tf.keras.metrics.Metric):\n",
    "    \"\"\"Computes the Weighted Error of the predictions with respect to the labels: FP + rate * FN\n",
    "\n",
    "    This metric creates two local variables, `false_negatives` and\n",
    "    `false_positives`, that are used to compute the weighted error. This value is\n",
    "  ultimately returned as `weighted-error`, an idempotent operation that simply multiplies\n",
    "    `false_negatives` by 'rate' and sums this value with `false_positives`.\n",
    "\n",
    "    If `sample_weight` is `None`, weights default to 1.\n",
    "    Use `sample_weight` of 0 to mask values.\n",
    "\n",
    "    If `top_k` is set, weighted error will be computed as how often on average a class\n",
    "    among the labels of a batch entry is in the top-k predictions.\n",
    "\n",
    "    If `class_id` is specified, we calculate recall by considering only the\n",
    "    entries in the batch for which `class_id` is in the label, and computing the\n",
    "    fraction of them for which `class_id` is above the threshold and/or in the\n",
    "    top-k predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               thresholds=None,\n",
    "               top_k=None,\n",
    "               class_id=None,\n",
    "               rate=None,\n",
    "               name='weighted-error',\n",
    "               dtype=None):\n",
    "        \"\"\"Creates a `weighted-error` instance.\n",
    "\n",
    "        Args:\n",
    "          thresholds: (Optional) A float value or a python list/tuple of float\n",
    "            threshold values in [0, 1]. A threshold is compared with prediction\n",
    "            values to determine the truth value of predictions (i.e., above the\n",
    "            threshold is `true`, below is `false`). One metric value is generated\n",
    "            for each threshold value. If neither thresholds nor top_k are set, the\n",
    "            default is to calculate recall with `thresholds=0.5`.\n",
    "          top_k: (Optional) Unset by default. An int value specifying the top-k\n",
    "            predictions to consider when calculating recall.\n",
    "          class_id: (Optional) Integer class ID for which we want binary metrics.\n",
    "            This must be in the half-open interval `[0, num_classes)`, where\n",
    "            `num_classes` is the last dimension of predictions.\n",
    "           rate: Integer class ID. The rate of the error.\n",
    "          name: (Optional) string name of the metric instance.\n",
    "          dtype: (Optional) data type of the metric result.\n",
    "        \"\"\"\n",
    "        super(WeightedError, self).__init__(name=name, dtype=dtype)\n",
    "        self.init_thresholds = thresholds\n",
    "        self.top_k = top_k\n",
    "        self.class_id = class_id\n",
    "        self.rate = rate\n",
    "\n",
    "        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n",
    "        self.thresholds = metrics_utils.parse_init_thresholds(\n",
    "            thresholds, default_threshold=default_threshold)\n",
    "        self.false_negatives = self.add_weight(\n",
    "            'false_negatives',\n",
    "            shape=(len(self.thresholds),),\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "        self.false_positives = self.add_weight(\n",
    "            'false_positives',\n",
    "            shape=(len(self.thresholds),),\n",
    "            initializer=init_ops.zeros_initializer)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Accumulates true positive and false negative statistics.\n",
    "\n",
    "        Args:\n",
    "          y_true: The ground truth values, with the same dimensions as `y_pred`.\n",
    "            Will be cast to `bool`.\n",
    "          y_pred: The predicted values. Each element must be in the range `[0, 1]`.\n",
    "          sample_weight: Optional weighting of each example. Defaults to 1. Can be a\n",
    "            `Tensor` whose rank is either 0, or the same rank as `y_true`, and must\n",
    "            be broadcastable to `y_true`.\n",
    "\n",
    "        Returns:\n",
    "          Update op.\n",
    "        \"\"\"\n",
    "        return metrics_utils.update_confusion_matrix_variables(\n",
    "            {\n",
    "                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,\n",
    "                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives\n",
    "            },\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            thresholds=self.thresholds,\n",
    "            top_k=self.top_k,\n",
    "            class_id=self.class_id,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        result = self.false_positives + math_ops.multiply_no_nan(self.false_negatives, self.rate)\n",
    "        return result[0] if len(self.thresholds) == 1 else result\n",
    "\n",
    "    def reset_states(self):\n",
    "        num_thresholds = len(to_list(self.thresholds))\n",
    "        K.batch_set_value(\n",
    "            [(v, np.zeros((num_thresholds,))) for v in self.variables])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'thresholds': self.init_thresholds,\n",
    "            'top_k': self.top_k,\n",
    "            'class_id': self.class_id,\n",
    "            'rate': self.rate\n",
    "        }\n",
    "        base_config = super(WeightedError, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_img(classifier):\n",
    "    ## Get the number of parameters of each model\n",
    "    total_params = np.sum([np.prod(v.get_shape().as_list()) for v in classifier.variables])\n",
    "    trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in classifier.trainable_variables])\n",
    "    return total_params,trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-bishop",
   "metadata": {},
   "source": [
    "## Proposed multi-input CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3-block model, one for each view\n",
    "def image_classifier_model_3blocks(a, b, c):\n",
    "    \"\"\"\n",
    "    CNN with frontal, L90 and R90 thermos of shape (a, b, c) and clinical data.\n",
    "    big_cd is True when there are more than 16 features, otherwise is False\n",
    "    \"\"\"\n",
    "\n",
    "    front_input = Input(shape=(a, b, c))\n",
    "    front = Conv2D(32, (3, 3), activation='relu', input_shape=(a, b, c))(front_input)\n",
    "    front = Conv2D(64, (5, 5), strides=2, activation='relu')(front)\n",
    "    front = MaxPooling2D((3, 3))(front)\n",
    "    front = Conv2D(64, (3, 3), activation='relu')(front)  \n",
    "    front = Conv2D(64, (5, 5), strides=2, activation='relu')(front)  \n",
    "    front = Conv2D(64, (3, 3), activation='relu')(front) \n",
    "    front = MaxPooling2D((2, 2))(front)\n",
    "    front = Conv2D(128, (3, 3), activation='relu')(front) \n",
    "    front = Conv2D(128, (3, 3), activation='relu')(front) # (5, 5), strides=2\n",
    "    front = Conv2D(128, (3, 3), activation='relu')(front)  # new\n",
    "    #front = MaxPooling2D((2, 2))(front)  \n",
    "    front = Conv2D(128, (5, 5), strides=2, activation='relu')(front) #strides=2\n",
    "    front = MaxPooling2D((2, 2))(front)  \n",
    "    front = Flatten()(front) \n",
    "    front = Dense(512, use_bias=False)(front)   #256\n",
    "    front = BatchNormalization()(front)\n",
    "    front = Activation('relu')(front)\n",
    "    front = Dropout(0.5)(front) \n",
    "    front = Dense(128, use_bias=False)(front) # activation='relu'\n",
    "    front = BatchNormalization()(front)\n",
    "    front = Activation('relu')(front)\n",
    "    front = Dropout(0.5)(front) \n",
    "    front = Dense(64, activation='relu')(front) \n",
    "    front = Dense(1, activation='sigmoid')(front) \n",
    "\n",
    "\n",
    "    L90_input = Input(shape=(a, b, c))\n",
    "    L90 = Conv2D(32, (3, 3), activation='relu', input_shape=(a, b, c))(L90_input)\n",
    "    L90 = Conv2D(32, (5, 5), strides=2, activation='relu')(L90) \n",
    "    L90 = MaxPooling2D((2, 2))(L90)\n",
    "    L90 = Conv2D(64, (3, 3), activation='relu')(L90) \n",
    "    L90 = Conv2D(64, (3, 3), activation='relu')(L90) \n",
    "    L90 = Conv2D(64, (3, 3), activation='relu')(L90) #new\n",
    "    L90 = MaxPooling2D((2, 2))(L90)\n",
    "    #L90 = Conv2D(128, (3, 3), activation='relu')(L90) \n",
    "    L90 = Conv2D(128, (3, 3), activation='relu')(L90)\n",
    "    L90 = Conv2D(128, (3, 3), activation='relu')(L90) \n",
    "    #L90 = MaxPooling2D((2, 2))(L90)  \n",
    "    L90 = Conv2D(128, (3, 3), activation='relu')(L90) \n",
    "    L90 = MaxPooling2D((2, 2))(L90)\n",
    "    L90 = Flatten()(L90)\n",
    "    L90 = Dense(512, use_bias=False)(L90)   #256\n",
    "    L90 = BatchNormalization()(L90)\n",
    "    L90 = Activation('relu')(L90)\n",
    "    L90 = Dropout(0.5)(L90) \n",
    "    L90 = Dense(128, use_bias=False)(L90) # activation='relu'\n",
    "    L90 = BatchNormalization()(L90)\n",
    "    L90 = Activation('relu')(L90)\n",
    "    L90 = Dropout(0.5)(L90) \n",
    "    L90 = Dense(64, activation='relu')(L90) \n",
    "    L90 = Dense(1, activation='sigmoid')(L90) \n",
    "\n",
    "    R90_input = Input(shape=(a, b, c))\n",
    "    R90 = Conv2D(32, (3, 3), activation='relu', input_shape=(a, b, c))(R90_input)\n",
    "    R90 = Conv2D(32, (5, 5), strides=2, activation='relu')(R90)   \n",
    "    R90 = MaxPooling2D((2, 2))(R90) \n",
    "    R90 = Conv2D(64, (3, 3), activation='relu')(R90)  \n",
    "    R90 = Conv2D(64, (3, 3), activation='relu')(R90) \n",
    "    R90 = Conv2D(64, (3, 3), activation='relu')(R90)  #new\n",
    "    R90 = MaxPooling2D((2, 2))(R90)\n",
    "    R90 = Conv2D(128, (3, 3), activation='relu')(R90) \n",
    "    R90 = Conv2D(128, (3, 3), activation='relu')(R90)\n",
    "    #R90 = Conv2D(128, (3, 3), activation='relu')(R90)  \n",
    "    #R90 = MaxPooling2D((2, 2))(R90)  \n",
    "    R90 = Conv2D(128, (3, 3), activation='relu')(R90)  \n",
    "    R90 = MaxPooling2D((2, 2))(R90) \n",
    "    R90 = Flatten()(R90)\n",
    "    R90 = Dense(512, use_bias=False)(R90)   #256\n",
    "    R90 = BatchNormalization()(R90)\n",
    "    R90 = Activation('relu')(R90)\n",
    "    R90 = Dropout(0.5)(R90) \n",
    "    R90 = Dense(128, use_bias=False)(R90) # activation='relu'\n",
    "    R90 = BatchNormalization()(R90)\n",
    "    R90 = Activation('relu')(R90)\n",
    "    R90 = Dropout(0.5)(R90) \n",
    "    R90 = Dense(64, activation='relu')(R90) \n",
    "    R90 = Dense(1, activation='sigmoid')(R90) \n",
    "\n",
    "    concatenated = concatenate([front, L90, R90], axis=-1)\n",
    "\n",
    "    X = Dense(1, activation='sigmoid')(concatenated)   \n",
    "\n",
    "    ## create the model\n",
    "    model = Model([front_input, L90_input, R90_input], X)  \n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-block model, with each view as an input channel\n",
    "def image_classifier_model_1block(a, b):\n",
    "    \"\"\"\n",
    "    CNN with frontal, L90 and R90 thermos of shape (a, b, c) and clinical data.\n",
    "    big_cd is True when there are more than 16 features, otherwise is False\n",
    "    \"\"\"\n",
    "\n",
    "    # Inputs\n",
    "    front_input = Input(shape=(a, b, 1))\n",
    "    L90_input = Input(shape=(a, b, 1))\n",
    "    R90_input = Input(shape=(a, b, 1))\n",
    "    stacked_input = tf.concat([front_input, L90_input, R90_input], axis=-1)\n",
    "    \n",
    "    #img_input = Input(shape=(a, b, c))\n",
    "    x = Conv2D(32, (3, 3), activation='relu', input_shape=(a, b, 3))(stacked_input)\n",
    "    x = Conv2D(64, (5, 5), strides=2, activation='relu')(x)\n",
    "    x = MaxPooling2D((3, 3))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)  \n",
    "    x = Conv2D(64, (5, 5), strides=2, activation='relu')(x)  \n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x) \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x) \n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x) # (5, 5), strides=2\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)  # new\n",
    "    #x = MaxPooling2D((2, 2))(x)  \n",
    "    x = Conv2D(128, (5, 5), strides=2, activation='relu')(x) #strides=2\n",
    "    x = MaxPooling2D((2, 2))(x)  \n",
    "    x = Flatten()(x) \n",
    "    x = Dense(512, use_bias=False)(x)   #256\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    x = Dense(128, use_bias=False)(x) # activation='relu'\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x) \n",
    "    x = Dense(64, activation='relu')(x) \n",
    "    x = Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "    X = Dense(1, activation='sigmoid')(x)   \n",
    "\n",
    "    ## create the model\n",
    "    model = Model([front_input, L90_input, R90_input], X)  \n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-nation",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model, front, L90, R90, cd, labels, k, EPOCHS=30):  \n",
    "    \n",
    "    ## Initialize model\n",
    "    init_weights = model.get_weights()\n",
    "    \n",
    "    ## Hyperparameters\n",
    "    # Learning rate\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    ## Cross-validation\n",
    "    train_val_subsets = create_crossval_subsets(front,L90,R90,cd,labels,k)\n",
    "    fold_loss = []\n",
    "    \n",
    "    for j,data in enumerate(train_val_subsets):\n",
    "        print('Fold {}/{}'.format(j+1, k))\n",
    "        \n",
    "        train_data,val_data = data\n",
    "        train_front,train_L90,train_R90,_,train_labels = train_data\n",
    "        val_front,val_L90,val_R90,_,val_labels = val_data\n",
    "        \n",
    "        n_sick = train_labels.sum()\n",
    "        n_healthy = len(train_labels) - n_sick\n",
    "        rate_train = n_healthy / n_sick\n",
    "        \n",
    "        # Set randomly initialized weights\n",
    "        model.set_weights(init_weights)\n",
    "        \n",
    "        # Optimizer\n",
    "        sgd = SGD(learning_rate=lr)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision',\n",
    "                                                           'Recall', Specificity(), WeightedError(rate=20)], optimizer=sgd)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)], train_labels,\n",
    "                            class_weight = {0: 1, 1: rate_train}, batch_size=8, epochs=EPOCHS, verbose=0,\n",
    "                            validation_data=([np.expand_dims(val_front,-1), np.expand_dims(val_L90,-1), np.expand_dims(val_R90,-1)], val_labels)) \n",
    "        \n",
    "        fold_loss.append(history.history['val_loss'][-1])\n",
    "        \n",
    "        print()\n",
    "            \n",
    "    return np.asarray(fold_loss).mean()  # Objective value linked with the Trial object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the objective inside a lambda and call objective inside it\n",
    "func = lambda trial: objective(trial, model, train_front, train_L90, train_R90, train_cd, train_labels, k, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3-block model\n",
    "print('3-block multi-input CNN for thermal image classification'), print()\n",
    "classifier_type = 'multiInputCNN_3blocks'\n",
    "\n",
    "## Initialize model\n",
    "model = image_classifier_model_3blocks(w, h, 1)\n",
    "\n",
    "## Tune hyperparameters (lr)\n",
    "tic = time.time()\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(func, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "tuningTime = time.time() - tic\n",
    "print('Hyperparameter tuning took',str(tuningTime//60),'mins and',str(tuningTime%60),'seconds')\n",
    "print('Best mean loss achieved:',str(study.best_value))\n",
    "print()\n",
    "\n",
    "## Get best hyperparameter combination\n",
    "lr = study.best_params['lr']\n",
    "tuned_lr = {classifier_type:lr}\n",
    "\n",
    "# Visualize the relationship between hyperparameters and the objective value.\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study, params=['lr'])\n",
    "fig.write_image(path+'Param_tuning/Param_selection_'+classifier_type+'.png')\n",
    "fig.show()\n",
    "\n",
    "# Visualize the objective values over the trials.\n",
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1-block model\n",
    "print('1-block multi-input CNN for thermal image classification'), print()\n",
    "classifier_type = 'multiInputCNN_1block'\n",
    "\n",
    "## Initialize model\n",
    "model = image_classifier_model_1block(w, h)\n",
    "\n",
    "## Tune hyperparameters (lr)\n",
    "tic = time.time()\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(func, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "tuningTime = time.time() - tic\n",
    "print('Hyperparameter tuning took',str(tuningTime//60),'mins and',str(tuningTime%60),'seconds')\n",
    "print('Best mean loss achieved:',str(study.best_value))\n",
    "print()\n",
    "\n",
    "## Get best hyperparameter combination\n",
    "lr = study.best_params['lr']\n",
    "tuned_lr[classifier_type] = lr\n",
    "\n",
    "# Visualize the relationship between hyperparameters and the objective value.\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study, params=['lr'])\n",
    "fig.write_image(path+'Param_tuning/Param_selection_curve_'+classifier_type+'.png')\n",
    "fig.show()\n",
    "\n",
    "# Visualize the objective values over the trials.\n",
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-persian",
   "metadata": {},
   "source": [
    "### Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiInputCNN_classifier(train_front,train_L90,train_R90,train_labels,test_front,test_L90,test_R90,test_labels,blocks=3,lr=0.0001,EPOCHS=30):  \n",
    "    \n",
    "    n_sick = train_labels.sum()\n",
    "    n_healthy = len(train_labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "\n",
    "    ## Initialize the model\n",
    "    if blocks == 3:\n",
    "        classifier = image_classifier_model_3blocks(w, h, 1)\n",
    "    else:\n",
    "        classifier = image_classifier_model_1block(w, h)\n",
    "        \n",
    "    ## Train the model\n",
    "    # Set the optimizer\n",
    "    sgd = SGD(learning_rate=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    classifier.compile(loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision',\n",
    "                                                            'Recall', Specificity(), WeightedError(rate=20)], optimizer=sgd)\n",
    "\n",
    "    # Train the model\n",
    "    tic = time.time()\n",
    "    history = classifier.fit([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)], train_labels,\n",
    "                              class_weight = {0: 1, 1: rate_train}, batch_size=8, epochs=EPOCHS, verbose=0,\n",
    "                              validation_data=([np.expand_dims(test_front,-1), np.expand_dims(test_L90,-1), np.expand_dims(test_R90,-1)], test_labels)) \n",
    "    train_time = time.time() - tic\n",
    "\n",
    "    ## Predictions\n",
    "    tic = time.time()\n",
    "    predictions_train = classifier.predict([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)])\n",
    "    inference_time = time.time() - tic\n",
    "\n",
    "    tic = time.time()\n",
    "    predictions_test = classifier.predict([np.expand_dims(test_front,-1), np.expand_dims(test_L90,-1), np.expand_dims(test_R90,-1)])\n",
    "    inference_time = inference_time + (time.time()-tic)\n",
    "\n",
    "    inference_time = inference_time/(len(predictions_train)+len(predictions_test))\n",
    "        \n",
    "    return classifier,history,train_time,inference_time,predictions_train,predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-sterling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 3-block multi-input CNN\n",
    "print('3-block multi-input CNN'), print()\n",
    "classifier_type = 'multiInputCNN_3blocks'\n",
    "\n",
    "# Train the model\n",
    "lr = tuned_lr[classifier_type] # Tuned value\n",
    "multiInput_3blocks,history,train_time,inference_time,predictions_train,predictions_test = train_multiInputCNN_classifier(train_front,train_L90,train_R90,train_labels,\n",
    "                                                                                                                         test_front,test_L90,test_R90,test_labels,\n",
    "                                                                                                                         blocks=3,lr=lr,EPOCHS=EPOCHS)\n",
    "\n",
    "# Save the model\n",
    "multiInput_3blocks.save(path+'Models/'+classifier_type+'.h5')\n",
    "\n",
    "# Save learning curves\n",
    "plot_training([history.history['loss'],history.history['accuracy'],history.history['auc']],\n",
    "              [history.history['val_loss'],history.history['val_accuracy'],history.history['val_auc']],EPOCHS)\n",
    "plt.savefig(path+'Learning_curves/'+classifier_type+'.png'), plt.show()\n",
    "    \n",
    "# Save predictions\n",
    "np.save(path+'Predictions/'+classifier_type+'_train.npy',predictions_train)\n",
    "np.save(path+'Predictions/'+classifier_type+'_test.npy',predictions_test)\n",
    "\n",
    "print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "print()\n",
    "print('Bootstrapping')\n",
    "print(bootstrapping(test_labels,predictions_test))\n",
    "print()\n",
    "print('Number of total parameters: ', num_parameters_img(multiInput_3blocks)[0])\n",
    "print('Number of trainable parameters: ', num_parameters_img(multiInput_3blocks)[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-block multi-input CNN\n",
    "print('1-block multi-input CNN'), print()\n",
    "classifier_type = 'multiInputCNN_1block'\n",
    "\n",
    "# Train the model\n",
    "lr = tuned_lr[classifier_type] # Tuned value\n",
    "multiInput_1block,history,train_time,inference_time,predictions_train,predictions_test = train_multiInputCNN_classifier(train_front,train_L90,train_R90,train_labels,\n",
    "                                                                                                                         test_front,test_L90,test_R90,test_labels,\n",
    "                                                                                                                         blocks=1,lr=lr,EPOCHS=EPOCHS)\n",
    "\n",
    "# Save the model\n",
    "multiInput_1block.save(path+'Models/'+classifier_type+'.h5'), plt.show()\n",
    "\n",
    "# Save learning curves\n",
    "plot_training([history.history['loss'],history.history['accuracy'],history.history['auc']],\n",
    "              [history.history['val_loss'],history.history['val_accuracy'],history.history['val_auc']],EPOCHS)\n",
    "plt.savefig(path+'Learning_curves/'+classifier_type+'.png')\n",
    "    \n",
    "# Save predictions\n",
    "np.save(path+'Predictions/'+classifier_type+'_train.npy',predictions_train)\n",
    "np.save(path+'Predictions/'+classifier_type+'_test.npy',predictions_test)\n",
    "\n",
    "print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "print()\n",
    "print('Bootstrapping')\n",
    "print(bootstrapping(test_labels,predictions_test))\n",
    "print()\n",
    "print('Number of total parameters: ', num_parameters_img(multiInput_1block)[0])\n",
    "print('Number of trainable parameters: ', num_parameters_img(multiInput_1block)[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-cleaning",
   "metadata": {},
   "source": [
    "## Pre-trained CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import densenet, inception_v3, mobilenet, mobilenet_v2, vgg16, vgg19, resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_dict = {'densenet121':'densenet.DenseNet121','densenet169':'densenet.DenseNet169','densenet201':'densenet.DenseNet201',\n",
    "            'mobilenet':'mobilenet.MobileNet','mobilenet_v2':'mobilenet_v2.MobileNetV2',\n",
    "            'vgg16':'vgg16.VGG16','vgg19':'vgg19.VGG19',\n",
    "            'resnet50':'resnet.ResNet50','resnet101':'resnet.ResNet101','resnet152':'resnet.ResNet152',\n",
    "            'inception_v3':'inception_v3.InceptionV3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraindCNN(model_handle, a, b):    \n",
    "    # Load a pretrained model (e.g., ResNet50)\n",
    "    baseModel = eval(model_handle + '(include_top=False, input_shape=(a,b,3))')\n",
    "    \n",
    "    # Replace top and most task-specifict layer\n",
    "    model = Sequential()\n",
    "    model.add(baseModel)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3-block model, one for each view\n",
    "def pretraindCNN_3blocks(model_handle,a,b):\n",
    "    \n",
    "    ## Front\n",
    "    front_input = Input(shape=(a, b, 1))\n",
    "    front = tf.repeat(front_input,3,-1)  # Convert to 3-channel image\n",
    "    front_model = pretraindCNN(model_handle, a, b)\n",
    "    front_model._name = 'front'\n",
    "    front = front_model(front)\n",
    "    \n",
    "    ## L90\n",
    "    L90_input = Input(shape=(a, b, 1))\n",
    "    L90 = tf.repeat(L90_input,3,-1)  # Convert to 3-channel image\n",
    "    L90_model = pretraindCNN(model_handle, a, b)\n",
    "    L90_model._name = 'L90'\n",
    "    L90 = L90_model(L90)\n",
    "    \n",
    "    ## R90\n",
    "    R90_input = Input(shape=(a, b, 1))\n",
    "    R90 = tf.repeat(R90_input,3,-1)  # Convert to 3-channel image\n",
    "    R90_model = pretraindCNN(model_handle, a, b)\n",
    "    R90_model._name = 'R90'\n",
    "    R90 = R90_model(R90)\n",
    "    \n",
    "    ## Ensemble\n",
    "    concatenated = concatenate([front, L90, R90], axis=-1)\n",
    "\n",
    "    X = Dense(1, activation='sigmoid')(concatenated)   \n",
    "\n",
    "    ## create the model\n",
    "    model = Model([front_input, L90_input, R90_input], X) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-block model, with each view as an input channel\n",
    "def pretraindCNN_1block(model_handle,a,b):\n",
    "    # Inputs\n",
    "    front_input = Input(shape=(a, b, 1))\n",
    "    L90_input = Input(shape=(a, b, 1))\n",
    "    R90_input = Input(shape=(a, b, 1))\n",
    "    stacked_input = tf.concat([front_input, L90_input, R90_input], axis=-1)\n",
    "    \n",
    "    # Model\n",
    "    model = pretraindCNN(model_handle, a, b)\n",
    "    X = model(stacked_input)\n",
    "    \n",
    "    ## create the model\n",
    "    model = Model([front_input, L90_input, R90_input], X)  \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freezeLayers(model,perc_unfreeze,baseModel_name):\n",
    "    for layer in model.layers:\n",
    "        if layer.name in ['front','L90','R90']:  # 3-block model\n",
    "            for layer2 in layer.layers:\n",
    "                if layer2.name == baseModel_name:\n",
    "                    L = len(layer2.layers)\n",
    "                    L_unfreeze = round(L*perc_unfreeze)\n",
    "                    for i,layer3 in enumerate(reversed(layer2.layers)):\n",
    "                        if (i+1) <= L_unfreeze:\n",
    "                            layer3.trainable = True\n",
    "                        else:\n",
    "                            layer3.trainable = False\n",
    "                else:\n",
    "                    layer2.trainable = True\n",
    "                    \n",
    "        elif layer.name == baseModel_name:  # 1-block model\n",
    "            L = len(layer.layers)\n",
    "            L_unfreeze = round(L*perc_unfreeze)\n",
    "            for i,layer2 in enumerate(reversed(layer.layers)):\n",
    "                if (i+1) <= L_unfreeze:\n",
    "                    layer2.trainable = True\n",
    "                else:\n",
    "                    layer2.trainable = False\n",
    "            else:\n",
    "                layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model,baseModel_name,train_front,train_L90,train_R90,train_labels,test_front,test_L90,test_R90,test_labels,\n",
    "             lr_freeze,lr_unfreeze,perc_unfreeze,EPOCHS):\n",
    "    ## Step 1: train randomly initialized weights\n",
    "    #num_epochs_freeze = num_epochs//2\n",
    "    print('Freezing base model...'), print()\n",
    "    \n",
    "    n_sick = train_labels.sum()\n",
    "    n_healthy = len(train_labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "    \n",
    "    # Optimizer\n",
    "    sgd = SGD(learning_rate=lr_freeze)\n",
    "    \n",
    "    # Freeze the base pretrained CNN\n",
    "    model = freezeLayers(model,0,baseModel_name)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision',\n",
    "                                                       'Recall', Specificity(), WeightedError(rate=20)], optimizer=sgd)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)], train_labels,\n",
    "                        class_weight={0: 1, 1: rate_train}, batch_size=8, epochs=EPOCHS//2, verbose=0,\n",
    "                        validation_data=([np.expand_dims(test_front,-1), np.expand_dims(test_L90,-1), np.expand_dims(test_R90,-1)], test_labels)) \n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    train_auc = history.history['auc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    val_auc = history.history['val_auc']\n",
    "    \n",
    "    print('Step 1 completed.'), print()\n",
    "    \n",
    "    ## Step 2: fine tune all the parameters in the model\n",
    "    print('Unfreezing base model...'), print()\n",
    "    \n",
    "    # Optimizer\n",
    "    sgd = SGD(learning_rate=lr_unfreeze)\n",
    "    \n",
    "    # Unfreeze the top perc_unfreeze% layers in the encoder\n",
    "    model = freezeLayers(model,perc_unfreeze,baseModel_name)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision',\n",
    "                                                       'Recall', Specificity(), WeightedError(rate=20)], optimizer=sgd)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)], train_labels,\n",
    "                        class_weight={0: 1, 1: rate_train}, batch_size=8, epochs=EPOCHS//2, verbose=0,\n",
    "                        validation_data=([np.expand_dims(test_front,-1), np.expand_dims(test_L90,-1), np.expand_dims(test_R90,-1)], test_labels)) \n",
    "    \n",
    "    train_loss = train_loss + history.history['loss']\n",
    "    train_acc = train_acc + history.history['accuracy']\n",
    "    train_auc = train_auc + history.history['auc']\n",
    "    val_loss = val_loss + history.history['val_loss']\n",
    "    val_acc = val_acc + history.history['val_accuracy']\n",
    "    val_auc = val_auc + history.history['val_auc']\n",
    "    \n",
    "    print('Step 2 completed.'), print()\n",
    "    \n",
    "    train_history = [train_loss,train_acc,train_auc]\n",
    "    val_history = [val_loss,val_acc,val_auc]\n",
    "    \n",
    "    return model, [train_history, val_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of top layers to unfreeze during second step of fine-tuning\n",
    "perc_unfreeze = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-faith",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model, front, L90, R90, cd, labels, k, EPOCHS=30, perc_unfreeze=0.2):  \n",
    "    \n",
    "    ## Initialize model\n",
    "    init_weights = model.get_weights()\n",
    "    \n",
    "    ## Hyperparameters\n",
    "    # Learning rates\n",
    "    lr_freeze = trial.suggest_float('lr_freeze', 1e-5, 1e-2, log=True)\n",
    "    lr_unfreeze = trial.suggest_float('lr_unfreeze', 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    ## Cross-validation\n",
    "    train_val_subsets = create_crossval_subsets(front,L90,R90,cd,labels,k)\n",
    "    fold_loss = []\n",
    "    \n",
    "    for j,data in enumerate(train_val_subsets):\n",
    "        print('Fold {}/{}'.format(j+1, k))\n",
    "        \n",
    "        train_data,val_data = data\n",
    "        train_front,train_L90,train_R90,_,train_labels = train_data\n",
    "        val_front,val_L90,val_R90,_,val_labels = val_data\n",
    "        \n",
    "        # Set randomly initialized weights\n",
    "        model.set_weights(init_weights)\n",
    "        \n",
    "        # Train the model\n",
    "        model, history = finetune(model,baseModel_name,train_front,train_L90,train_R90,train_labels,test_front,test_L90,test_R90,test_labels,\n",
    "                                  lr_freeze,lr_unfreeze,perc_unfreeze,EPOCHS)\n",
    "        _, val_history = history\n",
    "        val_loss,_,_ = val_history\n",
    "        \n",
    "        fold_loss.append(val_loss[-1])\n",
    "                    \n",
    "    return np.asarray(fold_loss).mean()  # Objective value linked with the Trial object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the objective inside a lambda and call objective inside it\n",
    "func_PT = lambda trial: objective(trial, model, train_front, train_L90, train_R90, train_cd, train_labels, k, EPOCHS, perc_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lr = {}\n",
    "\n",
    "## 3-CNN model\n",
    "for baseModel_name in CNN_dict.keys():\n",
    "    print(baseModel_name), print()\n",
    "    model_handle = CNN_dict[baseModel_name]\n",
    "    \n",
    "    #### 3-block model\n",
    "    print('Model with 3 pre-trained CNNs for thermal image classification'), print()\n",
    "    classifier_type = baseModel_name+'_3blocks'\n",
    "\n",
    "    ## Initialize model\n",
    "    model = pretraindCNN_3blocks(model_handle,w,h)\n",
    "\n",
    "    ## Tune hyperparameters (lr_freeze and lr_unfreeze)\n",
    "    tic = time.time()\n",
    "    study = optuna.create_study()  # Create a new study.\n",
    "    study.optimize(func_PT, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "    tuningTime = time.time() - tic\n",
    "    \n",
    "    print('Hyperparameter tuning took',str(tuningTime//60),'mins and',str(tuningTime%60),'seconds')\n",
    "    print('Best mean loss achieved:',str(study.best_value))\n",
    "    print()\n",
    "\n",
    "    ## Get best hyperparameter combination\n",
    "    lr_freeze = study.best_params['lr_freeze']\n",
    "    lr_unfreeze = study.best_params['lr_unfreeze']\n",
    "    tuned_lr[classifier_type] = [lr_freeze,lr_unfreeze]\n",
    "\n",
    "    # Visualize the relationship between hyperparameters and the objective value.\n",
    "    fig = optuna.visualization.plot_parallel_coordinate(study, params=['lr_freeze','lr_unfreeze'])\n",
    "    fig.write_image(path+'Param_tuning/Param_selection_pretrained_'+classifier_type+'.png')\n",
    "    fig.show()\n",
    "\n",
    "    # Visualize the objective values over the trials.\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "    \n",
    "    print(),print('#'*150),print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-CNN model\n",
    "for baseModel_name in CNN_dict.keys():\n",
    "    print(baseModel_name), print()\n",
    "    model_handle = CNN_dict[baseModel_name]\n",
    "    \n",
    "    #### 1-block model\n",
    "    print('Model with 1 pre-trained CNN for thermal image classification'), print()\n",
    "    classifier_type = baseModel_name+'_1block'\n",
    "\n",
    "    ## Initialize model\n",
    "    model = pretraindCNN_1block(model_handle,w,h)\n",
    "\n",
    "    ## Tune hyperparameters (lr_freeze and lr_unfreeze)\n",
    "    tic = time.time()\n",
    "    study = optuna.create_study()  # Create a new study.\n",
    "    study.optimize(func_PT, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "    tuningTime = time.time() - tic\n",
    "    \n",
    "    print('Hyperparameter tuning took',str(tuningTime//60),'mins and',str(tuningTime%60),'seconds')\n",
    "    print('Best mean loss achieved:',str(study.best_value))\n",
    "    print()\n",
    "\n",
    "    ## Get best hyperparameter combination\n",
    "    lr_freeze = study.best_params['lr_freeze']\n",
    "    lr_unfreeze = study.best_params['lr_unfreeze']\n",
    "    tuned_lr[classifier_type] = [lr_freeze,lr_unfreeze]\n",
    "\n",
    "    # Visualize the relationship between hyperparameters and the objective value.\n",
    "    fig = optuna.visualization.plot_parallel_coordinate(study, params=['lr_freeze','lr_unfreeze'])\n",
    "    fig.write_image(path+'Param_tuning/Param_selection_pretrained_'+classifier_type+'.png')\n",
    "    fig.show()\n",
    "\n",
    "    # Visualize the objective values over the trials.\n",
    "    optuna.visualization.plot_optimization_history(study)\n",
    "    \n",
    "    print(),print('#'*150),print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-navigator",
   "metadata": {},
   "source": [
    "### Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pretrainedCNN_classifier(baseModel_name,model_handle,train_front,train_L90,train_R90,train_labels,test_front,test_L90,test_R90,test_labels,\n",
    "                                   blocks=3,lr_freeze=0.0001,lr_unfreeze=0.00001,perc_unfreeze=0.2,EPOCHS=30):  \n",
    "    \n",
    "    ## Initialize the model\n",
    "    if blocks == 3:\n",
    "        classifier = pretraindCNN_3blocks(model_handle,w,h)\n",
    "    else:\n",
    "        classifier = pretraindCNN_1block(model_handle,w,h)\n",
    "        \n",
    "    # Train the model\n",
    "    tic = time.time()\n",
    "    classifier, history = finetune(classifier,baseModel_name,train_front,train_L90,train_R90,train_labels,test_front,test_L90,test_R90,test_labels,\n",
    "                                   lr_freeze,lr_unfreeze,perc_unfreeze,EPOCHS) \n",
    "    train_time = time.time() - tic\n",
    "\n",
    "    ## Predictions\n",
    "    tic = time.time()\n",
    "    predictions_train = classifier.predict([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)])\n",
    "    inference_time = time.time() - tic\n",
    "\n",
    "    tic = time.time()\n",
    "    predictions_test = classifier.predict([np.expand_dims(test_front,-1), np.expand_dims(test_L90,-1), np.expand_dims(test_R90,-1)])\n",
    "    inference_time = inference_time + (time.time()-tic)\n",
    "\n",
    "    inference_time = inference_time/(len(predictions_train)+len(predictions_test))\n",
    "        \n",
    "    return classifier,history,train_time,inference_time,predictions_train,predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3-CNN model\n",
    "for baseModel_name in CNN_dict.keys():\n",
    "    print(baseModel_name), print()\n",
    "    model_handle = CNN_dict[baseModel_name]\n",
    "    classifier_type = baseModel_name+'_3blocks'\n",
    "    \n",
    "    ## Tuned parameters\n",
    "    lr_freeze = tuned_lr[classifier_type][0]\n",
    "    lr_unfreeze = tuned_lr[classifier_type][1]\n",
    "        \n",
    "    ## Train the model\n",
    "    pretrained_3blocks,history,train_time,inference_time,predictions_train,predictions_test = train_pretrainedCNN_classifier(baseModel_name,model_handle,\n",
    "                                                                                                                             train_front,train_L90,train_R90,train_labels,\n",
    "                                                                                                                             test_front,test_L90,test_R90,test_labels,\n",
    "                                                                                                                             3,lr_freeze,lr_unfreeze,perc_unfreeze,EPOCHS)\n",
    "        \n",
    "    # Save history\n",
    "    train_history, val_history = history\n",
    "    plot_training(train_history,val_history,EPOCHS)\n",
    "    plt.savefig(path+'Learning_curves/'+classifier_type+'.png'), plt.show()\n",
    "\n",
    "    # Save model\n",
    "    pretrained_3blocks.save(path+'Models/'+classifier_type+'.h5')\n",
    "    \n",
    "    # Save predictions\n",
    "    np.save(path+'Predictions/'+classifier_type+'_train.npy',predictions_train)\n",
    "    np.save(path+'Predictions/'+classifier_type+'_test.npy',predictions_test)\n",
    "    \n",
    "    print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "    print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Bootstrapping')\n",
    "    print(bootstrapping(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Number of total parameters: ', num_parameters_img(pretrained_3blocks)[0])\n",
    "    print('Number of trainable parameters: ', num_parameters_img(pretrained_3blocks)[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-CNN model\n",
    "for baseModel_name in CNN_dict.keys():\n",
    "    print(baseModel_name), print()\n",
    "    model_handle = CNN_dict[baseModel_name]\n",
    "    classifier_type = baseModel_name+'_1blocks'\n",
    "    \n",
    "    ## Tuned parameters\n",
    "    lr_freeze = tuned_lr[classifier_type][0]\n",
    "    lr_unfreeze = tuned_lr[classifier_type][1]\n",
    "        \n",
    "    ## Train the model\n",
    "    pretrained_1block,history,train_time,inference_time,predictions_train,predictions_test = train_pretrainedCNN_classifier(baseModel_name,model_handle,\n",
    "                                                                                                                            train_front,train_L90,train_R90,train_labels,\n",
    "                                                                                                                            test_front,test_L90,test_R90,test_labels,\n",
    "                                                                                                                            1,lr_freeze,lr_unfreeze,perc_unfreeze,EPOCHS)\n",
    "        \n",
    "    # Save history\n",
    "    train_history, val_history = history\n",
    "    plot_training(train_history,val_history,EPOCHS)\n",
    "    plt.savefig(path+'Learning_curves/'+classifier_type+'.png'), plt.show()\n",
    "\n",
    "    # Save model\n",
    "    pretrained_1block.save(path+'Models/'+classifier_type+'.h5')\n",
    "    \n",
    "    # Save predictions\n",
    "    np.save(path+'Predictions/'+classifier_type+'_train.npy',predictions_train)\n",
    "    np.save(path+'Predictions/'+classifier_type+'_test.npy',predictions_test)\n",
    "    \n",
    "    print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "    print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Bootstrapping')\n",
    "    print(bootstrapping(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Number of total parameters: ', num_parameters_img(pretrained_1block)[0])\n",
    "    print('Number of trainable parameters: ', num_parameters_img(pretrained_1block)[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-making",
   "metadata": {},
   "source": [
    "## Compare with segmented frontal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_segmentation = ... # Set the path where the segmentation models and results are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_results = pd.read_csv(path_segmentation+'Results.csv',header=0)\n",
    "best_model = segmentation_results.iloc[np.argmax(segmentation_results['testDice'].values)]['pretrainedCNN']\n",
    "segmentation_model = tf.keras.models.load_model(path_segmentation+'Models/'+best_model+'.h5', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize = segmentation_model.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply segmentation model to frontal images\n",
    "\n",
    "# Train images\n",
    "train_front_reduced = np.asarray([cv2.resize(img,(dsize[2],dsize[1])) for img in train_front])  # Reduce image size to 240x320\n",
    "train_front_masks = segmentation_model(np.repeat(np.expand_dims(train_front_reduced,axis=-1),3,axis=-1))\n",
    "train_front_masks = np.asarray([cv2.resize(tf.squeeze(mask).numpy(),(train_front.shape[2],train_front.shape[1])) for mask in train_front_masks])  # Resize mask to original image shape\n",
    "train_front_segmented = np.multiply(train_front,train_front_masks)\n",
    "\n",
    "# Test images\n",
    "test_front_reduced = np.asarray([cv2.resize(img,(dsize[2],dsize[1])) for img in test_front])  # Reduce image size to 240x320\n",
    "test_front_masks = segmentation_model(np.repeat(np.expand_dims(test_front_reduced,axis=-1),3,axis=-1))\n",
    "test_front_masks = np.asarray([cv2.resize(tf.squeeze(mask).numpy(),(test_front.shape[2],test_front.shape[1])) for mask in test_front_masks])  # Resize mask to original image shape\n",
    "test_front_segmented = np.multiply(test_front,test_front_masks)\n",
    "\n",
    "# Display some examples\n",
    "N = 10\n",
    "idx = [np.random.randint(0,len(train_front)) for i in range(N)]\n",
    "display_list = [[train_front[i],train_front_masks[i]] for i in idx]\n",
    "plt.figure(figsize=(5*N//2,8))\n",
    "for i in range(N):\n",
    "    img = display_list[i][0]\n",
    "    mask = display_list[i][1]\n",
    "    plt.subplot(2, N//2, i+1)\n",
    "    plt.imshow(img,'gray'), plt.imshow(mask,alpha=0.5)            \n",
    "    plt.axis('off')\n",
    "plt.savefig(path+'Example_segmented_images.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Remove background in frontal and lateral images\n",
    "# Plot the histogram to select threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(np.concatenate((train_front,train_L90,train_R90,test_front,test_L90,test_R90)).flatten(), \n",
    "         bins=256, range=(0, 1), density=True, color='blue', alpha=0.7)\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "#threshold = input('Enter threshold value: ')\n",
    "#threshold = float(threshold)\n",
    "threshold = 0.41\n",
    "\n",
    "# Apply threshold to remove background\n",
    "train_front_segmented = np.multiply(train_front_segmented,train_front_segmented>threshold)\n",
    "test_front_segmented = np.multiply(test_front_segmented,test_front_segmented>threshold)\n",
    "train_L90_foreground = np.multiply(train_L90,train_L90>threshold)\n",
    "test_L90_foreground = np.multiply(test_L90,test_L90>threshold)\n",
    "train_R90_foreground = np.multiply(train_R90,train_R90>threshold)\n",
    "test_R90_foreground = np.multiply(test_R90,test_R90>threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-eight",
   "metadata": {},
   "source": [
    "### Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model, front, L90, R90, cd, labels, k, EPOCHS=30):  \n",
    "    \n",
    "    ## Initialize model\n",
    "    init_weights = model.get_weights()\n",
    "    \n",
    "    ## Hyperparameters\n",
    "    # Learning rate\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    ## Cross-validation\n",
    "    train_val_subsets = create_crossval_subsets(front,L90,R90,cd,labels,k)\n",
    "    fold_loss = []\n",
    "    \n",
    "    for j,data in enumerate(train_val_subsets):\n",
    "        print('Fold {}/{}'.format(j+1, k))\n",
    "        \n",
    "        train_data,val_data = data\n",
    "        train_front,train_L90,train_R90,_,train_labels = train_data\n",
    "        val_front,val_L90,val_R90,_,val_labels = val_data\n",
    "        \n",
    "        n_sick = train_labels.sum()\n",
    "        n_healthy = len(train_labels) - n_sick\n",
    "        rate_train = n_healthy / n_sick\n",
    "        \n",
    "        # Set randomly initialized weights\n",
    "        model.set_weights(init_weights)\n",
    "        \n",
    "        # Optimizer\n",
    "        sgd = SGD(learning_rate=lr)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='binary_crossentropy', metrics=['accuracy', 'AUC', 'TruePositives', 'FalseNegatives', 'TrueNegatives', 'FalsePositives', 'Precision',\n",
    "                                                           'Recall', Specificity(), WeightedError(rate=20)], optimizer=sgd)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit([np.expand_dims(train_front,-1), np.expand_dims(train_L90,-1), np.expand_dims(train_R90,-1)], train_labels,\n",
    "                            class_weight={0: 1, 1: rate_train}, batch_size=8, epochs=EPOCHS, verbose=0,\n",
    "                            validation_data=([np.expand_dims(val_front,-1), np.expand_dims(val_L90,-1), np.expand_dims(val_R90,-1)], val_labels)) \n",
    "        \n",
    "        fold_loss.append(history.history['val_loss'][-1])\n",
    "        \n",
    "        print()\n",
    "            \n",
    "    return np.asarray(fold_loss).mean()  # Objective value linked with the Trial object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the objective inside a lambda and call objective inside it\n",
    "func = lambda trial: objective(trial, model, train_front_segmented, train_L90_foreground ,train_R90_foreground, train_cd, train_labels, k, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_type = 'multiInputCNN_3blocks_segmented'\n",
    "\n",
    "## Initialize model\n",
    "model = image_classifier_model_3blocks(w, h, 1)\n",
    "\n",
    "## Tune hyperparameters (lr)\n",
    "tic = time.time()\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(func, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "tuningTime = time.time() - tic\n",
    "\n",
    "## Get best hyperparameter combination\n",
    "lr = study.best_params['lr']\n",
    "tuned_lr = {classifier_type:lr}\n",
    "\n",
    "# Visualize the relationship between hyperparameters and the objective value.\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study, params=['lr'])\n",
    "fig.write_image(path+'Param_tuning/Param_selection_curve_'+classifier_type+'.png')\n",
    "fig.show()\n",
    "\n",
    "# Visualize the objective values over the trials.\n",
    "optuna.visualization.plot_optimization_history(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-elevation",
   "metadata": {},
   "source": [
    "### Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_type = 'multiInputCNN_3blocks_segmented'\n",
    "\n",
    "# Train the model\n",
    "lr = tuned_lr[classifier_type] # Tuned value\n",
    "multiInput_3blocks_segmented,history,train_time,inference_time,predictions_train,predictions_test = train_multiInputCNN_classifier(train_front_segmented,train_L90_foreground,train_R90_foreground,train_labels,\n",
    "                                                                                                                                   test_front_segmented,test_L90_foreground,test_R90_foreground,test_labels,\n",
    "                                                                                                                                   blocks=3,lr=lr,EPOCHS=EPOCHS)\n",
    "\n",
    "# Save the model\n",
    "multiInput_3blocks_segmented.save(path+'Models/'+classifier_type+'.h5')\n",
    "\n",
    "# Save learning curves\n",
    "plot_training([history.history['loss'],history.history['accuracy'],history.history['auc']],\n",
    "              [history.history['val_loss'],history.history['val_accuracy'],history.history['val_auc']],EPOCHS)\n",
    "plt.savefig(path+'Learning_curves/'+classifier_type+'.png'), plt.show()\n",
    "    \n",
    "# Save predictions\n",
    "np.save(path+'Predictions/'+classifier_type+'_train.npy',predictions_train)\n",
    "np.save(path+'Predictions/'+classifier_type+'_test.npy',predictions_test)\n",
    "\n",
    "print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "print()\n",
    "print('Bootstrapping')\n",
    "print(bootstrapping(test_labels,predictions_test))\n",
    "print()\n",
    "print('Number of total parameters: ', num_parameters_img(multiInput_3blocks_segmented)[0])\n",
    "print('Number of trainable parameters: ', num_parameters_img(multiInput_3blocks_segmented)[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-overall",
   "metadata": {},
   "source": [
    "## Select best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best model to classify thermal images is multi-input CNN with 3 blocks, with one block for each view.')\n",
    "img_model = 'multiInputCNN_3blocks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-greek",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_weight(pred_CD, pred_img, labels, metric=None):\n",
    "    best_weight = 0\n",
    "    best_metric = 0\n",
    "    if metric == \"we\":\n",
    "        best_metric = np.inf\n",
    "            \n",
    "    for weight_cd in range(0, 10000, 1):\n",
    "        weight = weight_cd/10000\n",
    "                \n",
    "        new_pred = pred_CD * weight + pred_img * (1 - weight)\n",
    "        \n",
    "        if metric == \"accuracy\":\n",
    "            accuracy = accuracy_score(labels, np.round(new_pred))\n",
    "            if accuracy > best_metric:\n",
    "                best_metric = accuracy\n",
    "                best_weight = weight\n",
    "        elif metric == \"roc_auc\":\n",
    "            roc_auc = roc_auc_score(labels, new_pred)\n",
    "            if roc_auc > best_metric:\n",
    "                best_metric = roc_auc\n",
    "                best_weight = weight\n",
    "        elif metric == \"we\":\n",
    "            we = funcion_perdida_we(labels, np.round(new_pred))\n",
    "            if we < best_metric:\n",
    "                best_metric = we\n",
    "                best_weight = weight\n",
    "        else:\n",
    "            tune_weight(pred_CD, pred_img, labels, \"accuracy\")\n",
    "            tune_weight(pred_CD, pred_img, labels, \"roc_auc\")\n",
    "            tune_weight(pred_CD, pred_img, labels, \"we\")\n",
    "            break\n",
    "            \n",
    "    print(\"\\Best \", metric , \": \", best_metric, \" | Best weight: \", best_weight)\n",
    "    return best_weight\n",
    "\n",
    "\n",
    "def weighted_voting(pred_img,pred_cd,weight=0.698):\n",
    "    return pred_cd * weight + pred_img * (1 - weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['WeightedVoting','DT','SGD','NN','SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read predictions\n",
    "# Clinical data classifier\n",
    "pred_CD_train = np.load(path+'Predictions/cd_'+cd_model+'_train.npy')\n",
    "pred_CD_test = np.load(path+'Predictions/cd_'+cd_model+'_test.npy')\n",
    "    \n",
    "# Clinical data classifier\n",
    "pred_img_train = np.load(path+'Predictions/'+img_model+'_train.npy')\n",
    "pred_img_test = np.load(path+'Predictions/'+img_model+'_test.npy')\n",
    "\n",
    "# Concatenate predictions to generate the input vector\n",
    "inputs_train = np.concatenate((pred_img_train.reshape(-1, 1),pred_CD_train.reshape(-1, 1)), axis=1)\n",
    "inputs_test = np.concatenate((pred_img_test.reshape(-1, 1),pred_CD_test.reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-treasurer",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-short",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tune weight for weighted voting\n",
    "tic = time.time()\n",
    "weight = tune_weight(pred_CD_train, pred_img_train[:,0], train_labels, metric='accuracy')\n",
    "train_time_wv = time.time() - tic\n",
    "print('Tuned weight:',weight)\n",
    "ensemble_params['WeightedVoting'] = {'weight' : weight}\n",
    "\n",
    "# Evaluate\n",
    "pred_ensemble_train = weighted_voting(pred_img_train[:,0],pred_CD_train,weight)\n",
    "print('TRAIN'), print(numeric_results(train_labels,pred_ensemble_train)), print()\n",
    "pred_ensemble_test = weighted_voting(pred_img_test[:,0],pred_CD_test,weight)\n",
    "print('TEST:'), print(numeric_results(test_labels,pred_ensemble_test)), print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-agency",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Tune other classifier's parameters with 4-fold cross-validation\n",
    "# Tune models\n",
    "for classifier_type in models[1::]:\n",
    "    print(classifier_type), print()\n",
    "    \n",
    "    if classifier_type == 'DT':\n",
    "        estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'ccp_alpha' : np.arange(0, 0.1, 0.005),\n",
    "                      'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "                      'max_depth' : [None, 10, 20, 30, 40, 50], #[None, 1, 10, 15],  # Maximum depth of the tree\n",
    "                      'max_features': [None, 'auto', 'sqrt', 'log2'], #['auto'],  # Number of features to consider when looking for the best split\n",
    "                      'max_leaf_nodes': [None, 10, 20, 30, 40, 50], #[None, 3, 6, 9],  # Grow a tree with max_leaf_nodes in best-first fashion\n",
    "                      'min_samples_leaf': [1, 5, 10, 20], #[2, 3, 4],  # Minimum number of samples required to be at a leaf node\n",
    "                      'min_samples_split' : [2, 10, 20, 30], #[3, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "                      'min_weight_fraction_leaf' : np.arange(0.1, 0.5, 0.01),\n",
    "                      'splitter': ['best', 'random'],    # Strategy used to choose the split at each node\n",
    "                     }        \n",
    "    elif classifier_type == 'SVM':\n",
    "        estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "        param_grid = {'C': [0.1, 1, 10, 100], # range(1, 1000, 10), \n",
    "                      'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "                      #'degree': [2, 3, 4, 5],\n",
    "                      #'coef0': [0.0, 0.1, 0.5, 1.0],\n",
    "                      #'shrinking': [True, False],\n",
    "                      #'decision_function_shape': ['ovo', 'ovr'],\n",
    "                      'gamma': ['scale', 'auto', 0.001,  0.01, 0.1, 1], # 0.0001, 0.00001]\n",
    "                     }\n",
    "    elif classifier_type == 'SGD':\n",
    "        estimator = SGDClassifier(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                      'penalty': [None, 'l2', 'l1', 'elasticnet'],\n",
    "                      'alpha': [0.0001, 0.001, 0.01, 0.1], #, 1, 10],\n",
    "                      'l1_ratio': [0.15, 0.3, 0.5, 0.7, 0.9],  # Only used if penalty is 'elasticnet'\n",
    "                      'max_iter': [1000, 2000, 3000],\n",
    "                      'tol': [1e-3, 1e-4, 1e-5],\n",
    "                      'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "                      'eta0':  [0.001, 0.01, 0.1] #, 1, 10]  # Only used if learning_rate is 'constant', 'invscaling', or 'adaptive'\n",
    "                     }\n",
    "    else:\n",
    "        estimator = Perceptron(class_weight={0: 1, 1: rate_train})\n",
    "        param_grid = {'penalty': [None, 'l2', 'l1', 'elasticnet'],\n",
    "                      'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "                      'fit_intercept': [True, False],\n",
    "                      'max_iter': [1000, 2000, 3000, 4000, 5000],\n",
    "                      'tol': [1e-3, 1e-4, 1e-5],\n",
    "                      'shuffle': [True, False],\n",
    "                      'eta0': [0.1, 0.01, 0.001],\n",
    "                      'early_stopping': [True, False],\n",
    "                      'validation_fraction': [0.1, 0.2, 0.3],\n",
    "                      'n_iter_no_change': [5, 10, 20]\n",
    "                     }\n",
    "        #estimator = MLPClassifier(random_state = 1)\n",
    "        #param_grid = {'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "        #              'activation': ['tanh', 'relu', 'logistic'],\n",
    "        #              'solver': ['sgd', 'adam'],\n",
    "        #              'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "        #              'learning_rate': ['constant', 'adaptive'],\n",
    "        #              'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        #              'max_iter': [200, 500],\n",
    "        #              'batch_size': ['auto', 32, 64, 128],\n",
    "        #              'tol': [1e-4, 1e-3, 1e-2]\n",
    "        #             }\n",
    "    \n",
    "    classifier, parameters, results_cv = classifier_tuneHyperparameters(estimator, param_grid, inputs_train, train_labels)\n",
    "    ensemble_params[classifier_type] = parameters\n",
    "    \n",
    "    print('Tuned parameters:'), print(parameters), print()\n",
    "    \n",
    "    # Evaluate\n",
    "    print('TRAIN:'), print(numeric_results(train_labels,classifier.predict_proba(inputs_train)))\n",
    "    print('TEST:'), print(numeric_results(test_labels,classifier.predict_proba(inputs_test)))\n",
    "    \n",
    "    print('#'*150), print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-slovenia",
   "metadata": {},
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble_classifier(inputs_train,train_labels,inputs_test,test_labels,classifier_type):  \n",
    "    \n",
    "    n_sick = train_labels.sum()\n",
    "    n_healthy = len(train_labels) - n_sick\n",
    "    rate_train = n_healthy / n_sick\n",
    "    \n",
    "    ## Initialize the estimator\n",
    "    if classifier_type == 'SVM':\n",
    "        # Support Vector Machine (SVM) classifier\n",
    "        parameters = ensemble_params[classifier_type]\n",
    "        #parameters = {'C':1.0, 'kernel':'rbf', 'gamma':'scale'}\n",
    "        estimator = SVC(class_weight={0: 1, 1: rate_train}, probability=True)\n",
    "        \n",
    "    elif classifier_type == 'DT':\n",
    "        # Decision Tree (DT) classifier\n",
    "        parameters = ensemble_params[classifier_type]\n",
    "        #parameters = {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 3, \n",
    "        #              'min_samples_split': 10, 'min_weight_fraction_leaf': 0.13999999999999999, 'splitter': 'best'}\n",
    "        estimator = DecisionTreeClassifier(class_weight={0: 1, 1: rate_train})\n",
    "            \n",
    "    elif classifier_type == 'SGD':\n",
    "        # Linear classifier\n",
    "        parameters = ensemble_params[classifier_type]\n",
    "        #parameters = {'loss': 'modified_huber'}\n",
    "        estimator = SGDClassifier(class_weight={0: 1, 1: rate_train})\n",
    "            \n",
    "    else:\n",
    "        # Neural Network\n",
    "        parameters = ensemble_params[classifier_type]\n",
    "        estimator = Perceptron(class_weight={0: 1, 1: rate_train})\n",
    "        #estimator = MLPClassifier(random_state = 1)        \n",
    "            \n",
    "    estimator.set_params(**parameters)\n",
    "    \n",
    "        \n",
    "    ## Train the classifier\n",
    "    tic = time.time()\n",
    "    classifier = estimator.fit(inputs_train, train_labels)\n",
    "    train_time = time.time() - tic\n",
    "\n",
    "    ## Predictions\n",
    "    tic = time.time()\n",
    "    if classifier_type == 'NN':\n",
    "        predictions_train = classifier.predict(inputs_train)\n",
    "        predictions_test = classifier.predict(inputs_test)\n",
    "    else:\n",
    "        predictions_train = classifier.predict_proba(inputs_train)[:,1]\n",
    "        predictions_test = classifier.predict_proba(inputs_test)[:,1]\n",
    "    inference_time = (time.time()-tic)/(len(predictions_train)+len(predictions_test))\n",
    "        \n",
    "    return classifier,train_time,inference_time,predictions_train,predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weighted voting classifier\n",
    "print('WeightedVoting'), print()\n",
    "# Train the model\n",
    "tic = time.time()\n",
    "weight = tune_weight(pred_CD_train, pred_img_train[:,0], train_labels, metric='accuracy')\n",
    "train_time = time.time() - tic\n",
    "# Evaluate\n",
    "tic = time.time()\n",
    "predictions_train = weighted_voting(pred_img_train[:,0],pred_CD_train,weight)\n",
    "predictions_test = weighted_voting(pred_img_test[:,0],pred_CD_test,weight)\n",
    "inference_time = (time.time()-tic)/(len(predictions_train)+len(predictions_test))\n",
    "\n",
    "np.save(path+'Predictions/ensemble_WeightedVoting_train.npy',predictions_train)\n",
    "np.save(path+'Predictions/ensemble_WeightedVoting_test.npy',predictions_test)\n",
    "\n",
    "print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "print()\n",
    "print('Bootstrapping')\n",
    "print(bootstrapping(test_labels,predictions_test))\n",
    "print()\n",
    "print('Number of parameters: ', num_parameters_classifiers(classifier_type,classifier))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train classifiers\n",
    "for classifier_type in models[1::]:\n",
    "    print(classifier_type), print()\n",
    "        \n",
    "    ## Train the classifier\n",
    "    classifier,train_time,inference_time,predictions_train,predictions_test = train_ensemble_classifier(inputs_train,train_labels,\n",
    "                                                                                                        inputs_test,test_labels,\n",
    "                                                                                                        classifier_type, ensemble_params)\n",
    "    \n",
    "    ## Save the model\n",
    "    with open(path+'Models/ensemble_'+classifier_type+'.pkl','wb') as f:\n",
    "        pickle.dump(classifier,f)\n",
    "\n",
    "    ## Store predictions\n",
    "    np.save(path+'Predictions/ensemble_'+classifier_type+'_trai.npy',predictions_train)\n",
    "    np.save(path+'Predictions/ensemble_'+classifier_type+'_test.npy',predictions_test)\n",
    "    \n",
    "    print('TRAIN - ',str(train_time),'s'), print(numeric_results(train_labels,predictions_train))\n",
    "    print('TEST - ', str(inference_time),'s'), print(numeric_results(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Bootstrapping')\n",
    "    print(bootstrapping(test_labels,predictions_test))\n",
    "    print()\n",
    "    print('Number of parameters: ', num_parameters_classifiers(classifier_type,classifier))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-nerve",
   "metadata": {},
   "source": [
    "## Select best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = 'SVM'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
